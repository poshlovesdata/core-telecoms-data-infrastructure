{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Core Telecoms: Unified Customer Data Platform","text":"<p>A production-grade ELT pipeline designed to unify customer experience data, reduce churn, and enable 360\u00b0 analytics.</p>"},{"location":"#executive-summary","title":"Executive Summary","text":"<p>Core Telecoms, a leading US telecommunications provider, faced fragmented customer data across Postgres databases (Web Forms), CSV dumps (Call Center Logs), and JSON streams (Social Media), which delayed insights and increased churn.</p> <p>This project delivers a Unified Data Platform that:</p> <ul> <li>Ingests data from 5 disparate sources into a centralized Data Lake.</li> <li>Transforms messy, raw logs into clean, business-ready Marts using a Medallion Architecture.</li> <li>Automates the lifecycle with Airflow 3.0 and GitHub Actions.</li> <li>Secures PII and infrastructure using IAM Roles and S3 Encryption.</li> </ul> <p>Business Impact: Reduces report generation time from days to minutes, providing a single source of truth for Customer Success teams.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The platform follows a Lakehouse Architecture, decoupling storage (S3) from compute (Snowflake) for scalability and cost efficiency.</p>"},{"location":"#high-level-data-flow","title":"High-Level Data Flow","text":"<pre><code>Sources -&gt; Ingestion (Airflow) -&gt; Data Lake (S3) -&gt; Warehouse (Snowflake) -&gt; Transformation (dbt) -&gt; BI / Analytics\n</code></pre> <ul> <li>Extract: Python-based Airflow DAGs pull data from Spreadsheets, APIs, and Databases.</li> <li>Load: Parquet files are loaded into Snowflake RAW tables via COPY INTO.</li> <li>Transform: dbt models clean (Bronze), join (Silver), and aggregate (Gold) data.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li> <p>Management Presentation (Slide Deck)</p> </li> <li> <p>Data Lineage/Documentation</p> </li> </ul>"},{"location":"#architecture-decision-records-adrs","title":"Architecture Decision Records (ADRs)","text":"<p>Explore the detailed engineering decisions behind this platform:</p> <ul> <li> <p>001 - Infrastructure Setup: Cloud foundation, networking, and security.</p> </li> <li> <p>002 - Compute &amp; Security: Snowflake vs. Redshift and Identity Management.</p> </li> <li> <p>003 - Orchestration: Airflow 3.0 configuration and event-driven scheduling.</p> </li> <li> <p>004 - Static Ingestion: Handling Google Sheets and messy CSVs.</p> </li> <li> <p>005 - Dynamic Ingestion: Incremental extraction patterns for DBs and Logs.</p> </li> <li> <p>006 - Data Loading: Secure Snowflake-S3 integration.</p> </li> <li> <p>007 - Transformation: dbt Medallion architecture and data modeling.</p> </li> <li> <p>008 - CI/CD &amp; Automation: DevOps pipelines and quality gates.</p> </li> </ul>"},{"location":"#live-execution-graph-event-driven","title":"Live Execution Graph (Event-Driven)","text":"<p>This pipeline uses Airflow Assets (Data-Aware Scheduling) instead of brittle time-based dependencies. The Transformation layer only runs once the Snowflake data is confirmed loaded.</p>"},{"location":"#tech-stack-decisions","title":"Tech Stack &amp; Decisions","text":"Component Technology Engineering Decision Infrastructure Terraform Fully modular IaC. S3 Native State Locking for concurrency safety. Zero-click deployment. Orchestration Apache Airflow 3.0 Adopted v3.0 (API Server architecture). LocalExecutor for efficient resource usage. Implemented Data-Aware Scheduling (Assets) for event-driven dependencies (Ingest -&gt; Load -&gt; Transform), eliminating race conditions. Data Lake AWS S3 (Parquet) Schema-on-Write via PyArrow. Snappy compression reduces storage costs ~60%. Lifecycle Policies enabled for auto-archiving. Warehouse Snowflake Separation of storage/compute. Connected via Storage Integrations (no hardcoded keys). Transformation dbt Core Medallion Architecture (Bronze/Silver/Gold). Outputs: Star Schema + One Big Table (OBT). Observability Slack Integration Real-time failure alerts sent to Slack channels via Airflow callbacks (on_failure_callback).on_failure_callback. CI/CD GitHub Actions Automated Quality Gates: <code>ruff</code> (Python), <code>terraform validate</code>, and <code>dbt parse</code> on every PR. Docker build/push on merge."},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 airflow/               # Orchestration Layer\n\u2502   \u251c\u2500\u2500 dags/              # Python DAGs (Extract &amp; Load)\n\u2502   \u251c\u2500\u2500 common/            # Reusable Python Utilities (S3Ingestor)\n\u2502   \u251c\u2500\u2500 Dockerfile         # Custom Airflow 3.0 Image\n\u2502   \u2514\u2500\u2500 docker-compose.yaml # Local Dev Environment\n\u251c\u2500\u2500 dbt/                   # Transformation Layer\n\u2502   \u251c\u2500\u2500 models/            # SQL Models (Staging, Int, Marts)\n\u2502   \u251c\u2500\u2500 profiles.yml       # Connection Config (Env Var Bridge)\n\u2502   \u2514\u2500\u2500 dbt_project.yml    # Project Config\n\u251c\u2500\u2500 terraform/             # Infrastructure Layer\n\u2502   \u251c\u2500\u2500 main.tf            # AWS Resource Definitions\n\u2502   \u251c\u2500\u2500 iam.tf             # Least Privilege Roles &amp; Policies\n\u2502   \u2514\u2500\u2500 modules/           # Reusable Modules (VPC, S3)\n\u251c\u2500\u2500 scripts/               # Helper Scripts (Snowflake DDL)\n\u2514\u2500\u2500 docs/                  # Architecture Decision Records (ADRs)\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose</li> <li>AWS Account (Free Tier friendly)</li> <li>Snowflake Account (Standard Edition)</li> <li>Google Cloud Project (with Sheets API enabled)</li> <li>Terraform CLI (v1.5+)</li> <li>Slack Workspace (for alerts)</li> </ul>"},{"location":"#1-provision-infrastructure","title":"1. Provision Infrastructure","text":"<pre><code>cd terraform\nterraform init\nterraform apply -var=\"snowflake_account_arn=...\"\n# See docs/006 for Snowflake handshake steps\n</code></pre>"},{"location":"#2-configure-data-warehouse","title":"2. Configure Data Warehouse","text":"<p>Run the SQL scripts to initialize Snowflake RBAC, Warehouses, and Storage Integrations.</p> <ol> <li> <p>Account Setup: Run <code>scripts/snowflake/01_snowflake_setup.sql</code> as ACCOUNTADMIN.</p> </li> <li> <p>Raw Layer &amp; Handshake: Run <code>scripts/snowflake/02_setup_raw_layer.sql</code>.</p> </li> <li> <p>Note: Ensure you update the STORAGE_AWS_ROLE_ARN with the output from Terraform.</p> </li> </ol>"},{"location":"#2-configure-local-environment","title":"2. Configure Local Environment","text":"<p>Create <code>.env</code> in <code>airflow/</code>:</p> <pre><code>AWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=..\nGOOGLE_SHEET_ID=..\nAIRFLOW_CONN_SNOWFLAKE_DEFAULT=snowflake://...\nAIRFLOW_CONN_SLACK_DEFAULT=slack://xoxb-your-token...\n</code></pre>"},{"location":"#3-launch-the-platform","title":"3. Launch the Platform","text":"<pre><code>cd airflow\ndocker-compose up --build -d\n</code></pre> <p>Access Airflow UI at <code>http://localhost:8080</code> (User: admin / Pass: airflow)</p>"},{"location":"#4-run-the-pipeline","title":"4. Run the Pipeline","text":"<ul> <li>Trigger Ingestion DAGs: <code>01_static</code>, <code>02_postgres</code>, <code>03_s3</code> \u2192 populate S3</li> <li>Trigger Loading DAG: <code>04_load_raw</code> \u2192 hydrate Snowflake</li> <li>Trigger Transformation DAG: <code>05_dbt</code> \u2192 build Marts</li> </ul>"},{"location":"#security-governance","title":"Security &amp; Governance","text":"<ul> <li>Network Security: S3 buckets enforce <code>block_public_acls</code>.</li> <li>Identity: Hybrid Identity Model: IAM Roles for Production, IAM Users for Local Dev.</li> <li>Secrets: Zero secrets in Git; all credentials via Environment Variables or AWS SSM Parameter Store.</li> <li>Metadata: <code>_ingested_at</code> and <code>_loaded_at</code> timestamps for full lineage traceability.</li> <li>FinOps: Lifecycle Rules automatically transition data to Infrequent Access (30 days) and Glacier (90 days) to optimize long-term storage costs.</li> <li>Resilience: Pipeline enables catchup=True to automatically backfill historical data from the source inception date, ensuring no gaps in data coverage.</li> </ul>"},{"location":"#data-quality-dbt-tests","title":"Data Quality (dbt Tests)","text":"<p>Pipeline enforces strict data contracts:</p> <ul> <li>Uniqueness: Primary keys (<code>customer_id</code>, <code>complaint_id</code>) must be unique.</li> <li>Not Null: Critical foreign keys required.</li> <li>Accepted Values: Status columns validated (<code>Resolved</code>, <code>Open</code>).</li> </ul> <p>Run tests manually:</p> <pre><code>docker exec -it airflow-apiserver bash -c \"cd dbt &amp;&amp; dbt test\"\n</code></pre>"},{"location":"#author","title":"Author","text":"<p>Oluwapelumi Oshundiya \u2013 Data Platform Engineer LinkedIn</p> <p>Built as a Capstone Project demonstrating Data Engineering competencies.</p>"},{"location":"architecture_decisions/001_infrastructure_setup/","title":"Day 1: Cloud Infrastructure &amp; Foundation","text":""},{"location":"architecture_decisions/001_infrastructure_setup/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 1 was to establish a secure, reproducible, and scalable cloud foundation for the Core Telecoms data platform. I successfully provisioned a Virtual Private Cloud (VPC) in eu-north-1 (Stockholm) and a tiered Data Lake architecture using Terraform (Infrastructure as Code).</p> <p>The priority was to balance Production-Grade Security with Cost Optimization for a zero-revenue capstone environment.</p>"},{"location":"architecture_decisions/001_infrastructure_setup/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/001_infrastructure_setup/#a-infrastructure-as-code-iac-strategy","title":"A. Infrastructure as Code (IaC) Strategy","text":"<ul> <li>Decision: Adopted a Modular Terraform Architecture rather than a monolithic <code>main.tf</code> file.</li> <li>Justification: Separation of concerns. Networking (<code>vpc</code>), Storage (<code>s3</code>), and Identity (<code>iam</code>) are decoupled. Allows updates to the network without risking the state of data storage.</li> <li>Tooling: Implemented pre-commit hooks to enforce formatting (<code>terraform fmt</code>) and linting before code enters the repository, ensuring high code quality from Day 1.</li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#b-the-zero-cost-network-architecture","title":"B. The \"Zero-Cost\" Network Architecture","text":"<ul> <li>Context: A standard enterprise VPC uses NAT Gateways in private subnets to allow outbound internet access.</li> <li>Constraint: NAT Gateways cost ~$32/month per Availability Zone.</li> <li>Decision: Deployed a standard VPC (Public/Private subnets) but disabled NAT Gateways.</li> <li>Impact: Resources in Private Subnets are currently air-gapped from the internet.</li> <li>Mitigation: Compute resources requiring internet access (e.g., installing Python packages) will be placed in Public Subnets secured by strict Security Groups restricting access to specific IPs. Maintains security without the monthly cost.</li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#c-modern-state-management-locking","title":"C. Modern State Management (Locking)","text":"<ul> <li>Context: Terraform requires a lock to prevent concurrent infrastructure modifications that can corrupt the state file.</li> <li>Decision: Leveraged S3 Native Locking (<code>use_lockfile = true</code>) instead of the legacy DynamoDB approach.</li> <li>Justification: Utilizes strong consistency features released by AWS in 2024/2025, simplifying the infrastructure stack and maintaining safety.</li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#d-data-lake-security-the-iron-dome","title":"D. Data Lake Security (The \"Iron Dome\")","text":"<ul> <li>Decision: Enforced Server-Side Encryption (SSE-S3) and Public Access Blocks on all S3 buckets.</li> <li> <p>Justification: \"Secure by Default.\" Even in a demo environment, data leaks are unacceptable.</p> </li> <li> <p><code>block_public_acls = true</code>: Prevents accidental public uploads.</p> </li> <li> <p><code>restrict_public_buckets = true</code>: Overrides any bad bucket policies.</p> </li> <li> <p>Result: Data Lake is inaccessible from the public internet, ensuring compliance with data privacy standards.</p> </li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#e-finops-storage-lifecycle-management","title":"E. FinOps: Storage Lifecycle Management","text":"<ul> <li> <p>Context: Data accumulates over time, increasing storage costs. Most analytics queries target recent data (\"Hot\"), while older data is rarely accessed (\"Cold\").</p> </li> <li> <p>Decision: Implemented an S3 Lifecycle Policy via Terraform.</p> </li> <li> <p>Transition to Standard-IA: After 30 days (Save ~40%).</p> </li> <li> <p>Transition to Glacier IR: After 90 days (Save ~80%).</p> </li> <li> <p>Cleanup: Delete non-current versions (ghost files) after 7 days.</p> </li> <li> <p>Justification: Automates cost optimization without manual intervention. Ensures the platform remains cost-effective as data volume grows, aligning with enterprise FinOps best practices.</p> </li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#3-naming-conventions-tagging","title":"3. Naming Conventions &amp; Tagging","text":"<ul> <li> <p>Resource Pattern: <code>cde-[project]-[resource]</code></p> </li> <li> <p>Example: <code>cde-core-telecoms-datalake</code></p> </li> <li> <p>Tagging Strategy:</p> </li> <li> <p><code>ManagedBy = Terraform</code>: Indicates immutable infrastructure.</p> </li> <li><code>Environment = Dev</code>: Allows cost allocation and automated cleanup.</li> <li><code>Owner = DataEngineering</code>: Identifies the responsible team.</li> </ul>"},{"location":"architecture_decisions/001_infrastructure_setup/#4-technical-trade-off-analysis","title":"4. Technical Trade-Off Analysis","text":"Feature Production Approach Capstone (Current) Approach Reasoning Network Multi-AZ NAT Gateways No NAT Gateway Saves ~$64/month. Complexity handled via Security Groups. Storage <code>force_destroy = false</code> <code>force_destroy = true</code> Allows rapid iteration/teardown during development. Environment Multi-Account (Dev/Prod) Single Account (Variable-based) Reduces overhead of managing multiple AWS accounts for a single engineer."},{"location":"architecture_decisions/001_infrastructure_setup/#5-next-steps","title":"5. Next Steps","text":"<p>With the foundation secured, the immediate next phase is Compute Provisioning. I will deploy Snowflake (via Free Trial) as the Data Warehouse to leverage its separation of storage and compute, bypassing networking complexities of Redshift in a NAT-less environment.</p>"},{"location":"architecture_decisions/002_compute_and_security/","title":"Day 2: Compute Engine &amp; Identity Architecture","text":""},{"location":"architecture_decisions/002_compute_and_security/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 2 was to provision the Data Warehouse (Compute) and establish the Identity Access Management (IAM) layer for the data pipeline. I successfully configured Snowflake as the central data warehouse and implemented a Hybrid Identity Model in AWS to support both local development (Docker) and future production deployment (EC2/ECS) without code changes.</p>"},{"location":"architecture_decisions/002_compute_and_security/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/002_compute_and_security/#a-compute-engine-snowflake-vs-redshift","title":"A. Compute Engine: Snowflake vs. Redshift","text":"<ul> <li>Context: The project requires a scalable SQL engine to transform data.</li> <li>Constraint: Current VPC (eu-north-1) does not have a NAT Gateway, making private Redshift cluster connectivity difficult.</li> <li>Decision: Selected Snowflake (Standard Edition) on AWS.</li> <li> <p>Justification:</p> </li> <li> <p>Connectivity: Operates as SaaS over public HTTPS, no NAT Gateway or complex VPC Endpoints required.</p> </li> <li>Cost: Leveraged 30-day Free Trial ($400 credits), avoiding Redshift costs.</li> <li>Separation of Compute &amp; Storage: Ideal for bursty batch processing pipelines.</li> </ul>"},{"location":"architecture_decisions/002_compute_and_security/#b-identity-management-the-hybrid-parity-pattern","title":"B. Identity Management: The \"Hybrid Parity\" Pattern","text":"<ul> <li>Context: Pipeline runs locally in Docker but is designed for EC2/ECS production.</li> <li>Challenge: Local containers need Access Keys; production servers should use IAM Roles.</li> <li> <p>Decision: Implemented \"One Policy, Two Identities\" pattern in Terraform.</p> </li> <li> <p>Resource 1 (Prod): <code>aws_iam_role</code> with Trust Policy for <code>ec2.amazonaws.com</code></p> </li> <li>Resource 2 (Local): <code>aws_iam_user</code> with programmatic Access Keys</li> <li> <p>Shared Logic: Both identities attached to the same <code>aws_iam_policy</code> (<code>core-telecoms-airflow-policy</code>).</p> </li> <li> <p>Impact: Guarantees environment parity; code working locally ensures production functionality.</p> </li> </ul> <p>View the configuration here: iam.tf</p>"},{"location":"architecture_decisions/002_compute_and_security/#c-secrets-management-ssm-vs-secrets-manager","title":"C. Secrets Management: SSM vs. Secrets Manager","text":"<ul> <li>Context: Airflow needs to store Snowflake password and database credentials securely.</li> <li>Decision: Selected AWS Systems Manager (SSM) Parameter Store (Standard).</li> <li> <p>Justification:</p> </li> <li> <p>Cost: SSM Standard parameters are free; Secrets Manager costs $0.40/secret/month.</p> </li> <li>Security: <code>SecureString</code> type ensures encryption at rest.</li> <li> <p>Implementation Detail: \"Lifecycle Ignore\" pattern in Terraform:</p> <ul> <li>Terraform creates resource with dummy value (<code>CHANGE_ME</code>).</li> <li>Actual secret updated manually in Console.</li> <li>Terraform ignores drift (<code>lifecycle { ignore_changes = [value] }</code>) preventing overwrites.</li> </ul> </li> </ul> <p>View the configuration here: ssm.tf</p>"},{"location":"architecture_decisions/002_compute_and_security/#3-security-governance","title":"3. Security &amp; Governance","text":"<ul> <li> <p>IAM Least Privilege Scope:</p> </li> <li> <p>S3: <code>GetObject</code>, <code>PutObject</code>, <code>ListBucket</code> only on <code>core-telecoms-data-lake</code> bucket.</p> </li> <li> <p>SSM: <code>GetParameter</code> only on paths starting with <code>/core_telecoms/*</code>.</p> </li> <li> <p>Result: Even if Airflow credentials were compromised, attackers cannot spin up EC2 instances or delete unrelated infrastructure.</p> </li> </ul> <p>View the configuration here: iam.tf</p>"},{"location":"architecture_decisions/002_compute_and_security/#4-technical-trade-off-analysis","title":"4. Technical Trade-Off Analysis","text":"Feature Production Enterprise Approach Capstone (Current) Approach Reasoning Secrets AWS Secrets Manager (Auto-Rotation) SSM Parameter Store Saves cost; auto-rotation is overkill for a short-term capstone. Compute Network AWS PrivateLink (VPC Endpoint) Public Internet (TLS 1.2) PrivateLink costs ~$30/month; TLS is secure enough for non-PII data. Dev Auth AWS SSO / IAM Identity Center IAM User (Access Keys) Simplifies local Docker setup without complex SSO integration."},{"location":"architecture_decisions/002_compute_and_security/#5-next-steps","title":"5. Next Steps","text":"<p>With the Compute engine ready and Identity secured, the next phase is Orchestration Setup. I will configure the local Airflow environment (Docker) to use the IAM User keys and connect to the Snowflake Warehouse.</p>"},{"location":"architecture_decisions/003_orchestration_setup/","title":"Day 3: Orchestration &amp; Local Environment Architecture","text":""},{"location":"architecture_decisions/003_orchestration_setup/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 3 was to establish the local orchestration environment. I containerized Apache Airflow 3.0 using Docker and configured it to interact securely with the AWS (S3) and Snowflake resources provisioned in previous phases.</p> <p>The architecture prioritizes Developer Experience (DevEx) and Resource Efficiency for a single-machine setup while adopting the latest Airflow 3.0 architectural standards (AIP-44).</p>"},{"location":"architecture_decisions/003_orchestration_setup/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/003_orchestration_setup/#a-airflow-version","title":"A. Airflow Version","text":"<ul> <li>Context: Airflow 3.0 introduces architectural changes, separating the Execution API Server from the Scheduler/Webserver.</li> <li>Decision: Adopted Airflow 3.0.6.</li> <li> <p>Justification:</p> </li> <li> <p>Future-Proofing: Building on the latest standard avoids immediate technical debt.</p> </li> <li>Workload Isolation: Leverages the new <code>api-server</code> component, aligning with AIP-44 standards.</li> </ul>"},{"location":"architecture_decisions/003_orchestration_setup/#b-executor-strategy-localexecutor-vs-celeryexecutor","title":"B. Executor Strategy: LocalExecutor vs. CeleryExecutor","text":"<ul> <li>Context: Production recommends CeleryExecutor or KubernetesExecutor, which require message brokers and separate workers.</li> <li>Constraint: Local MacBook (~16GB RAM) must run Snowflake/AWS connectors simultaneously.</li> <li>Decision: Selected LocalExecutor.</li> <li> <p>Justification:</p> </li> <li> <p>Resource Efficiency: Eliminates redis, airflow-worker, and flower containers (~4GB RAM saved).</p> </li> <li>Simplicity: Tasks run as subprocesses on the scheduler/machine.</li> <li>Parity: Sufficient parallelism for non-distributed capstone workload.</li> </ul> <p>View the configuration here: docker-compose.yaml</p>"},{"location":"architecture_decisions/003_orchestration_setup/#c-hybrid-cloud-authentication","title":"C. Hybrid Cloud Authentication","text":"<ul> <li>Context: Airflow container needs authentication with AWS S3 and Google Cloud (Sheets API).</li> <li> <p>Decision: Implemented Environment Variable Injection.</p> </li> <li> <p>AWS: Injected <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> from host <code>.env</code> file.</p> </li> <li> <p>GCP: Mounted <code>google_service_account.json</code> to <code>/opt/airflow/secrets/</code>.</p> </li> <li> <p>Security Measure: Added <code>.env</code> and <code>secrets/*.json</code> to <code>.gitignore</code>.</p> </li> </ul>"},{"location":"architecture_decisions/003_orchestration_setup/#d-scheduling-strategy-event-driven-assets","title":"D. Scheduling Strategy: Event-Driven (Assets)","text":"<ul> <li> <p>Context: The pipeline consists of dependent stages (Ingest -&gt; Load -&gt; Transform).</p> </li> <li> <p>Challenge: Using time-based scheduling (e.g., all DAGs running <code>@daily</code> at 7 AM) creates race conditions. If Ingestion is slow, the Loading DAG might start processing empty/old data.</p> </li> <li> <p>Decision: Implemented Data-Aware Scheduling (Airflow Assets).</p> </li> </ul> <p></p> <ul> <li> <p>Mechanism:</p> </li> <li> <p>Producers: Ingestion DAGs define <code>outlets=[Asset(\"s3://...\")]</code> to signal completion.</p> </li> <li> <p>Consumers: Downstream DAGs (Load/Transform) define <code>schedule=[Asset(\"...\")]</code> to wake up only when the upstream data is ready.</p> </li> <li> <p>Impact: Eliminates race conditions and ensures the Transformation layer never runs until the Snowflake Loading is confirmed complete.</p> </li> </ul>"},{"location":"architecture_decisions/003_orchestration_setup/#e-observability-real-time-alerts-slack","title":"E. Observability: Real-Time Alerts (Slack)","text":"<ul> <li> <p>Context: Pipeline failures must be detected immediately to prevent data freshness delays.</p> </li> <li> <p>Options:</p> </li> <li> <p>SMTP (Email): Traditional, verbose, requires managing App Passwords and SMTP host configuration.</p> </li> <li> <p>Slack Webhook: Instant, team-centric, simpler authentication via tokens.</p> </li> <li> <p>Decision: Selected Slack Integration via <code>on_failure_callback</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Implementation:</p> </li> <li> <p>Created a shared utility common.utils.task_failure_alert.</p> </li> <li> <p>Uses <code>SlackAPIPostOperator</code> (Bot Token Auth) to send formatted blocks containing the specific Task ID, DAG ID, and Log URL.</p> </li> <li> <p>Justification: Slack provides faster \"Time to Detect\" (TTD) than email. Using a Bot Token allows for granular permission scopes (<code>chat:write</code>) compared to a broad webhook, and integrates seamlessly with Airflow connections.</p> </li> </ul>"},{"location":"architecture_decisions/003_orchestration_setup/#3-infrastructure-components","title":"3. Infrastructure Components","text":"Service Role Configuration Postgres Metadata Store v13, Alpine-based for size. Persisted via Docker Volume API Server Execution Interface Airflow 3.0 component. Exposes port 8080. Scheduler Orchestrator LocalExecutor mode. Handles task triggering. Triggerer Async Events Handles Deferrable Operators (Sensors). DAG Processor Parser Decoupled parsing logic for stability."},{"location":"architecture_decisions/003_orchestration_setup/#4-next-steps","title":"4. Next Steps","text":"<p>With the engine running, the next phase is Data Ingestion (Day 4). I will write the first DAG to extract data from the \"Static\" sources (Google Sheets) and load it into the S3 Raw Zone.</p>"},{"location":"architecture_decisions/004_static_data_ingestion/","title":"Day 4: Static Data Ingestion Architecture","text":""},{"location":"architecture_decisions/004_static_data_ingestion/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 4 was to implement the Extract &amp; Load (EL) pipeline for static reference datasets. I successfully built an Airflow DAG (<code>01_ingest_static_data</code>) that ingests:</p> <ul> <li>Agent Data: From a private Google Sheet.</li> <li>Customer Data: From a messy CSV in an external S3 bucket.</li> </ul> <p>The solution enforces a Schema-On-Write approach, converting all raw inputs into standardized, compressed Parquet files in the S3 Raw Zone. </p>"},{"location":"architecture_decisions/004_static_data_ingestion/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/004_static_data_ingestion/#a-ingestion-pattern-modular-utility-s3ingestor","title":"A. Ingestion Pattern: Modular Utility (S3Ingestor)","text":"<ul> <li>Context: Pipeline logic (download \u2192 parse \u2192 clean \u2192 upload) is repetitive across multiple sources.</li> <li>Decision: Encapsulated logic into a reusable class <code>common.s3_utils.S3Ingestor</code>.</li> <li> <p>Justification:</p> </li> <li> <p>DRY Principle: Prevents code duplication across DAGs.</p> </li> <li>Standardization: Enforces uniform Parquet settings (<code>Snappy</code> compression, PyArrow engine) and metadata injection (<code>_ingested_at</code>) for all files.</li> </ul> <p>View code here : s3_utils.py</p>"},{"location":"architecture_decisions/004_static_data_ingestion/#b-performance-strategy-pyarrow-with-fallback","title":"B. Performance Strategy: PyArrow with Fallback","text":"<ul> <li>Context: <code>Customers.csv</code> is large and contains messy formatting (multiline addresses).</li> <li>Challenge: Pandas parser is slow; PyArrow engine crashes on malformed rows.</li> <li> <p>Decision: Implemented an Optimistic Performance Pattern.</p> </li> <li> <p>Try Fast: Parse with <code>engine=\"pyarrow\"</code> (multithreaded, ~10x faster).</p> </li> <li> <p>Catch &amp; Recover: On <code>ArrowInvalid</code>, fallback to standard C engine with <code>on_bad_lines='warn'</code>.</p> </li> <li> <p>Impact: Maximizes speed for clean files while ensuring stability for messy legacy data.</p> </li> </ul>"},{"location":"architecture_decisions/004_static_data_ingestion/#c-technical-hygiene-schema-standardization","title":"C. Technical Hygiene: Schema Standardization","text":"<ul> <li>Context: Source headers contain trailing spaces causing downstream failures.</li> <li>Decision: Automated column sanitization during extraction.</li> <li>Logic: <code>re.sub(r'[^a-zA-Z0-9]', '_', col.lower().strip())</code></li> <li>Justification: Raw layer remains immutable; structure must be valid for downstream consumption.</li> </ul>"},{"location":"architecture_decisions/004_static_data_ingestion/#d-file-format-parquet-vs-csvjson","title":"D. File Format: Parquet vs. CSV/JSON","text":"<ul> <li>Context: Project requirements specify raw data storage.</li> <li>Decision: Convert all inputs to Parquet immediately.</li> <li> <p>Justification:</p> </li> <li> <p>Cost: Snappy-compressed Parquet reduces S3 storage costs by ~60%.</p> </li> <li>Type Safety: Preserves schema information, preventing type inference errors in Snowflake.</li> </ul>"},{"location":"architecture_decisions/004_static_data_ingestion/#3-infrastructure-security","title":"3. Infrastructure &amp; Security","text":"Component Configuration Note Google Sheets GoogleSheetsHook Authenticated via Service Account JSON mounted in Docker. S3 Source boto3 Client Authenticated via AWS IAM User keys injected from host <code>.env</code>. Network download_file to /tmp Decouples network latency from CPU parsing. Temp files auto-cleaned."},{"location":"architecture_decisions/004_static_data_ingestion/#4-next-steps","title":"4. Next Steps","text":"<p>With static data ingestion complete and the reusable S3Ingestor tested, the next phase is Dynamic Ingestion (Day 5). I will implement the logic to handle daily incremental loads from Postgres and S3 log streams.</p>"},{"location":"architecture_decisions/005_dynamic_ingestion/","title":"Day 5: Dynamic Ingestion Architecture","text":""},{"location":"architecture_decisions/005_dynamic_ingestion/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 5 was to implement the Incremental Extraction layer. Unlike static data, these sources generate new data daily. I implemented dynamic Airflow DAGs that use the <code>logical_date</code> to target specific daily slices of data from Postgres and S3.</p>"},{"location":"architecture_decisions/005_dynamic_ingestion/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/005_dynamic_ingestion/#a-idempotency-via-logical-dates","title":"A. Idempotency via Logical Dates","text":"<ul> <li>Context: Daily data is stored in files/tables with date suffixes (e.g., <code>_2025_11_20</code>).</li> <li>Decision: Used Airflow's <code>logical_date</code> to calculate the target path.</li> <li> <p>Impact: Pipeline is fully idempotent.</p> </li> <li> <p>Re-running the DAG for \"Nov 20th\" today correctly targets that date, not today's data.</p> </li> <li>Enables safe backfilling and replays.</li> </ul>"},{"location":"architecture_decisions/005_dynamic_ingestion/#b-resilience-automated-backfilling","title":"B. Resilience: Automated Backfilling","text":"<ul> <li> <p>Context: The project requirements specified extracting data from a historical start date (Nov 20th, 2025), not just \"today onwards\".</p> </li> <li> <p>Decision: Enabled Airflow's Catchup mechanism.</p> </li> <li> <p><code>start_date = datetime(2025, 11, 20)</code></p> </li> <li> <p><code>catchup = True</code></p> </li> <li> <p>Justification:</p> </li> <li> <p>Completeness: Automatically generates DAG runs for every day between the start date and the current date, ensuring no historical data gaps.</p> </li> <li> <p>Consistency: The exact same code logic handles both \"history\" (backfill) and \"future\" (scheduled runs), adhering to the DRY principle.</p> </li> </ul>"},{"location":"architecture_decisions/005_dynamic_ingestion/#c-postgres-extraction-pattern","title":"C. Postgres Extraction Pattern","text":"<ul> <li>Challenge: Source Postgres table name changes daily (<code>Web_form_request_YYYY_MM_DD</code>).</li> <li> <p>Solution:</p> </li> <li> <p>Dynamic SQL: Construct table names using f-strings: <code>f\"web_form_request_{logical_date.strftime('%Y_%m_%d')}\"</code></p> </li> <li>Pre-Flight Check: Query <code>information_schema.tables</code> to ensure table exists.</li> <li>Soft Fail: Missing tables log a warning and skip (Success state), handling late-arriving data gracefully.</li> </ul> <p>View code: 02_ingest_daily_postgres.py</p> <p></p>"},{"location":"architecture_decisions/005_dynamic_ingestion/#d-s3-extraction-pattern","title":"D. S3 Extraction Pattern","text":"<ul> <li>Challenge: Source logs stored as daily CSV/JSON files.</li> <li> <p>Solution:</p> </li> <li> <p>Used <code>boto3</code> with <code>s3.download_file</code> + <code>pd.read_csv</code>.</p> </li> <li> <p>Performance Optimization:</p> <ul> <li>Streaming via <code>obj['Body']</code> for smaller files (&lt;300MB) to reduce I/O latency.</li> <li>Leveraged reusable <code>S3Ingestor</code> for schema hygiene (sanitized column names) and metadata injection (<code>_ingested_at</code>).</li> </ul> </li> </ul> <p>View code: 02_ingest_daily_postgres.py</p> <p></p>"},{"location":"architecture_decisions/005_dynamic_ingestion/#e-partitioning-strategy","title":"E. Partitioning Strategy","text":"<ul> <li>Context: Store data in S3 Raw for efficient downstream processing and easy navigation.</li> <li> <p>Decision: Partitioned by Year/Month/Day.</p> </li> <li> <p>Path: <code>s3://.../raw/web_forms/YYYY/MM/DD/data.parquet</code></p> </li> <li> <p>Justification:</p> </li> <li> <p>Simplicity: Cleaner, easier to browse path structure in AWS Console.</p> </li> <li>Performance: Enables partition pruning in Snowflake/External Tables using path prefixes.</li> </ul>"},{"location":"architecture_decisions/005_dynamic_ingestion/#3-next-steps","title":"3. Next Steps","text":"<p>With the Data Lake (Raw Layer) fully populated, the next phase is Data Loading (Day 6). I will configure Snowflake to ingest these Parquet files from S3 into the RAW schema tables.</p>"},{"location":"architecture_decisions/006_data_loading_snowflake/","title":"Day 6: Data Loading Architecture","text":""},{"location":"architecture_decisions/006_data_loading_snowflake/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 6 was to implement the Loading layer (L in ELT). I configured the Snowflake Data Warehouse to ingest Parquet files from the S3 Data Lake into the RAW schema tables using a secure, automated pipeline.</p> <p>Warehouse configuration script: snowflake</p> <p>View code: 04_load_raw_snowflake.py</p>"},{"location":"architecture_decisions/006_data_loading_snowflake/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/006_data_loading_snowflake/#a-authentication-storage-integration-secure-handshake","title":"A. Authentication: Storage Integration (Secure Handshake)","text":"<ul> <li>Context: Snowflake needs permission to read files from the private S3 Data Lake bucket.</li> <li>Anti-Pattern: Hardcoding AWS Access Keys directly in <code>CREATE STAGE</code> is insecure.</li> <li>Decision: Implemented a Storage Integration object.</li> <li> <p>Mechanism:</p> </li> <li> <p>AWS: Created IAM Role (<code>core-telecoms-snowflake-role</code>) with read access to the Data Lake bucket.</p> </li> <li>Snowflake: Created Storage Integration (<code>cde_s3_int</code>) pointing to the Role ARN.</li> <li> <p>Handshake: Retrieved <code>STORAGE_AWS_IAM_USER_ARN</code> and <code>STORAGE_AWS_EXTERNAL_ID</code> from Snowflake and updated AWS Role Trust Policy to allow only that specific Snowflake identity.</p> </li> <li> <p>Benefit: Zero long-lived credentials; authentication handled seamlessly via AWS STS.</p> </li> </ul> <p>View the configuration here: iam.tf</p>"},{"location":"architecture_decisions/006_data_loading_snowflake/#b-loading-strategy-copy-into-vs-snowpipe","title":"B. Loading Strategy: COPY INTO vs. Snowpipe","text":"<ul> <li>Context: Daily batch loads are required.</li> <li> <p>Options:</p> </li> <li> <p>Snowpipe: Event-driven, excellent for streaming.</p> </li> <li> <p>COPY INTO (Batch): Scheduled SQL loading.</p> </li> <li> <p>Decision: Selected <code>COPY INTO</code> orchestrated by Airflow (<code>SQLExecuteQueryOperator</code>).</p> </li> <li> <p>Justification:</p> </li> <li> <p>Cost Control: Snowpipe costs per file; batch loading utilizes warehouse credits efficiently.</p> </li> <li>Orchestration Dependency: Ensures transformations run only after successful load, managed via Airflow.</li> </ul>"},{"location":"architecture_decisions/006_data_loading_snowflake/#c-schema-handling-explicit-tables-vs-schema-on-read","title":"C. Schema Handling: Explicit Tables vs. Schema-on-Read","text":"<ul> <li>Context: Parquet files contain embedded schema metadata.</li> <li>Decision: Defined explicit DDL for raw tables with <code>MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE</code>.</li> <li> <p>Justification:</p> </li> <li> <p>Contract: Guarantees <code>_ingested_at</code> exists.</p> </li> <li>Flexibility: Column reordering in Parquet files won't break the load.</li> </ul>"},{"location":"architecture_decisions/006_data_loading_snowflake/#d-idempotency-partition-pruning","title":"D. Idempotency: Partition Pruning","text":"<ul> <li> <p>Challenge 1: Preventing duplicate data ingestion if the DAG is re-run for a past date.</p> </li> <li> <p>Solution: The loading SQL dynamically targets specific S3 partitions based on the date path.</p> </li> <li> <p>Challenge 2: Event-driven DAGs (triggered by Assets) do not have a time-based schedule, so Airflow 3.0 removes standard time macros like ds and logical_date from the Jinja context, causing UndefinedError during template rendering.   </p> </li> <li> <p>Solution: Parsed the date directly from the unique run_id string, which always contains the timestamp.</p> </li> <li> <p>Implementation: <code>{{ run_id.split('__')[1][:10].replace('-', '/') }}</code> converts <code>asset_triggered\\_\\_2025-11-20...</code> into <code>2025/11/20</code>.</p> </li> <li> <p>Impact: We maintain idempotency and partition pruning even in an event-driven architecture where standard time context is absent.      </p> </li> </ul>"},{"location":"architecture_decisions/006_data_loading_snowflake/#3-infrastructure-configuration","title":"3. Infrastructure &amp; Configuration","text":"Component Resource Name Note AWS Role core-telecoms-snowflake-role Trusts Snowflake's IAM User/External ID. Snowflake Stage coretelecom_s3_stage External stage pointing to <code>s3://core-telecoms-data-lake/</code>. Airflow Connection snowflake_default Configured via <code>AIRFLOW_CONN_SNOWFLAKE_DEFAULT</code>."},{"location":"architecture_decisions/006_data_loading_snowflake/#4-next-steps","title":"4. Next Steps","text":"<p>With the data successfully loaded into Snowflake RAW tables, the Data Lakehouse is populated. The final phase is Transformation (Day 7). I will use dbt to apply business logic, clean the data, and model it into business-ready Marts.</p>"},{"location":"architecture_decisions/007_transformation_dbt/","title":"Day 7: Data Transformation Architecture","text":""},{"location":"architecture_decisions/007_transformation_dbt/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 7 was to implement the Transformation layer (T in ELT). I integrated dbt (data build tool) into the Airflow pipeline to clean, standardize, and model raw data residing in Snowflake.</p> <p>The solution adopts a Medallion Architecture (Bronze/Silver/Gold) to separate technical hygiene from business logic, culminating in both a Star Schema and a One Big Table (OBT) to serve diverse analytical needs.</p> <p></p>"},{"location":"architecture_decisions/007_transformation_dbt/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/007_transformation_dbt/#a-architecture-medallion-schema-bronze-silver-gold","title":"A. Architecture: Medallion Schema (Bronze \u2192 Silver \u2192 Gold)","text":"<ul> <li>Context: Raw data is messy and normalized; business users need clean, denormalized data.</li> <li> <p>Decision: Implemented a 3-layer architecture:</p> </li> <li> <p>Staging (Bronze): 1:1 views of raw tables. Focus on technical hygiene (renaming columns, casting types, standardizing formatting like lowercase emails).</p> </li> <li>Intermediate (Silver): \"Engine Room\". Integrates disparate sources (Web, Call, Social) into a unified <code>int_complaints_unioned</code> model and enriches with dimensions (<code>int_complaints_enriched</code>).</li> <li>Marts (Gold): \"Product\". Exposes business-ready entities (<code>dim_customers</code>, <code>fct_complaints</code>).</li> </ul>"},{"location":"architecture_decisions/007_transformation_dbt/#b-modeling-strategy-hybrid-star-schema-obt","title":"B. Modeling Strategy: Hybrid (Star Schema + OBT)","text":"<ul> <li> <p>Context: Different tools have different needs:</p> </li> <li> <p>PowerBI/Looker: Prefer Star Schemas.</p> </li> <li> <p>Tableau/Excel/Ad-hoc SQL: Prefer denormalized tables.</p> </li> <li> <p>Decision: Built both.</p> </li> <li> <p>Star Schema: <code>dim_customers</code>, <code>dim_agents</code>, <code>fct_complaints</code>.</p> </li> <li> <p>OBT: <code>obt_complaints_overview</code> (single wide table with all dimensions pre-joined).</p> </li> <li> <p>Justification: Maximizes flexibility without duplicating logic, since OBT selects from the enriched intermediate model.</p> </li> </ul>"},{"location":"architecture_decisions/007_transformation_dbt/#c-schema-management-custom-schemas-vs-public","title":"C. Schema Management: Custom Schemas vs. Public","text":"<ul> <li>Context: dbt defaults to the public schema.</li> <li>Challenge: Public schema in Snowflake is owned by <code>ACCOUNTADMIN</code>, causing permission errors.</li> <li>Decision: Configured dbt (<code>profiles.yml</code>) to target a dedicated <code>ANALYTICS</code> schema.</li> <li>Benefit: Cleaner separation of duties. Core data engineering role owns the <code>ANALYTICS</code> namespace, preventing permission conflicts.</li> </ul>"},{"location":"architecture_decisions/007_transformation_dbt/#d-data-quality-lineage","title":"D. Data Quality &amp; Lineage","text":"<ul> <li> <p>Decision: Propagated metadata through all layers:</p> </li> <li> <p><code>_ingested_at</code>: Preserved from S3 Raw to Gold (extraction time).</p> </li> <li><code>_loaded_at</code>: Preserved from Snowflake Raw to Gold (warehouse load time).</li> <li> <p><code>_enriched_at</code>: Added in Intermediate layer (transformation time).</p> </li> <li> <p>Impact: Full lineage observability; trace records from dashboard back to source extraction time.</p> </li> </ul>"},{"location":"architecture_decisions/007_transformation_dbt/#3-infrastructure-components","title":"3. Infrastructure Components","text":"Component Configuration Note dbt Core v1.8+ (Dockerized) Installed inside Airflow image for simplified orchestration. Orchestrator BashOperator Airflow triggers dbt run commands. Authentication Bridge Pattern Credentials fetched dynamically from Airflow Connections and injected into dbt as Environment Variables. No secrets stored in git."},{"location":"architecture_decisions/007_transformation_dbt/#4-next-steps","title":"4. Next Steps","text":"<p>With the Transformation layer complete, the Data Platform is functionally finished (Extract \u2192 Load \u2192 Transform). The final phase is Day 8: CI/CD &amp; Automation, where I will set up a CI pipeline (GitHub Actions) to lint SQL/Python code and verify build integrity on every Pull Request.</p>"},{"location":"architecture_decisions/008_cicd_automation/","title":"Day 8: CI/CD &amp; Automation Architecture","text":""},{"location":"architecture_decisions/008_cicd_automation/#1-executive-summary","title":"1. Executive Summary","text":"<p>The goal of Day 8 was to implement Continuous Integration and Continuous Delivery (CI/CD). I established automated quality gates to ensure that no broken code reaches the production branch (main) and automated the artifact delivery process.</p> <p>The pipeline is built on GitHub Actions, offering a zero-cost, fully integrated DevOps workflow.</p>"},{"location":"architecture_decisions/008_cicd_automation/#2-key-design-decisions","title":"2. Key Design Decisions","text":""},{"location":"architecture_decisions/008_cicd_automation/#a-ci-strategy-the-quality-gate","title":"A. CI Strategy: The \"Quality Gate\"","text":"<p>Context: We need to verify code quality across three languages (Python, HCL, SQL/Jinja) before merging.</p> <p>Decision: Implemented a multi-stage CI pipeline running in parallel:</p> <ul> <li>Python: <code>ruff</code> for ultra-fast linting of Airflow DAGs</li> <li>Terraform: <code>terraform validate</code> to check syntax &amp; configuration correctness</li> <li>dbt: <code>dbt parse</code> to validate SQL/YAML structure and Jinja references</li> </ul>"},{"location":"architecture_decisions/008_cicd_automation/#b-dbt-validation-parse-vs-compile","title":"B. dbt Validation: Parse vs. Compile","text":"<p>Challenge: Running <code>dbt compile</code> requires Snowflake connectivity, leading to security concerns (secrets in CI), cost implications, and slower pipelines.</p> <p>Decision: Use dbt parse.</p> <p>Justification:</p> <ul> <li>Security: No live database connection required</li> <li>Speed: Completes in seconds</li> <li>Trade-off: Logical SQL errors (invalid columns) are caught during developer testing, not CI</li> </ul>"},{"location":"architecture_decisions/008_cicd_automation/#c-cd-strategy-immutable-artifacts","title":"C. CD Strategy: Immutable Artifacts","text":"<p>Context: Manual Airflow updates are error-prone and lead to environment drift.</p> <p>Decision: Implemented a Docker Build &amp; Push workflow triggered on merge to <code>main</code>.</p> <p>Mechanism:</p> <ul> <li>Build a custom Airflow image with dependencies pre-installed</li> <li>Tag with both the Git SHA (<code>:abc1234</code>) and <code>:latest</code></li> <li>Push to Docker Hub</li> </ul> <p>Impact: Ensures production code is bit-for-bit identical to the tested code, eliminating \u201cworks on my machine\u201d issues.</p>"},{"location":"architecture_decisions/008_cicd_automation/#d-cd-optimization-smart-path-filtering","title":"D. CD Optimization: Smart Path Filtering","text":"<p>Context: Building Docker images takes 2\u20133 minutes. Rebuilding when only docs or Terraform change wastes resources.</p> <p>Decision: Enable path filtering using <code>dorny/paths-filter</code>.</p> <p>Logic:</p> <ul> <li>Docker Build &amp; Push runs **only when <code>airflow/**</code> changes**</li> <li>Docs deployment runs on every push</li> </ul> <p>Impact: Reduces CI/CD runtime by ~40% for non-code commits.</p>"},{"location":"architecture_decisions/008_cicd_automation/#e-documentation-automation-embedded-lineage","title":"E. Documentation Automation: Embedded Lineage","text":"<p>Context: Stakeholders need visibility into how data models connect\u2014not just SQL files.</p> <p>Decision: Integrate dbt docs generate in the CD pipeline.</p> <p>Mechanism:</p> <ol> <li>CD pipeline injects Snowflake credentials securely</li> <li>Executes <code>dbt docs generate</code></li> <li>Copies artifacts into <code>docs/dbt/</code> inside the MkDocs site</li> </ol> <p>Result: A single URL hosts:</p> <ul> <li>Architecture Decision Records (ADRs)</li> <li>End-to-end Lineage Graph</li> <li>dbt documentation</li> </ul>"},{"location":"architecture_decisions/008_cicd_automation/#3-pipeline-configuration","title":"3. Pipeline Configuration","text":"Workflow Trigger Jobs Tools CI Pull Request python-lint, terraform-validate, dbt-validate Ruff, Terraform CLI, dbt-core CD Push to Main build-and-push (conditional), deploy-docs (always) Docker Buildx, Docker Hub, MkDocs"},{"location":"architecture_decisions/008_cicd_automation/#4-final-project-status","title":"4. Final Project Status","text":"<p>With the completion of Day 8, the Core Telecoms Data Platform is fully automated and production-ready.</p> <ul> <li>Infrastructure: Provisioned &amp; secured (Terraform)</li> <li>Orchestration: Containerized &amp; scheduled (Airflow 3)</li> <li>Data Lake: Hydrated &amp; partitioned (S3)</li> <li>Warehouse: Modeled &amp; served (Snowflake / dbt)</li> <li>Operations: Automated (GitHub Actions)</li> </ul>"}]}